{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b72b0fd3",
   "metadata": {},
   "source": [
    "# Criar Dataset de Áudio\n",
    "Neste Jupyter Notebook é possível criar um dataset de áudio.\n",
    "\n",
    "Para isso, são separados diversas frases coletadas de um documento (por exemplo um PDF) para falar no microfone e salvar as frases e áudio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28df0304",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber, os, pathlib\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "from transformers import AutoModelForCTC, AutoProcessor, Wav2Vec2Processor\n",
    "import sys\n",
    "import librosa\n",
    "import pyaudio\n",
    "import IPython\n",
    "import webrtcvad\n",
    "import pyaudio\n",
    "import wave"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1412cd",
   "metadata": {},
   "source": [
    "Caso queria ver o nome de todos microfones disponíveis com terminada configuração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "289b931d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDA Intel: ALC897 Analog (hw:0,0)\n",
      "HDA Intel: ALC897 Alt Analog (hw:0,2)\n",
      "HyperX SoloCast: USB Audio (hw:1,0)\n",
      "sysdefault\n",
      "pulse\n",
      "default\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ALSA lib pcm_dmix.c:1052:(snd_pcm_dmix_open) unable to open slave\n",
      "ALSA lib pcm.c:2495:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.rear\n",
      "ALSA lib pcm.c:2495:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.center_lfe\n",
      "ALSA lib pcm.c:2495:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.side\n",
      "ALSA lib pcm_route.c:867:(find_matching_chmap) Found no matching channel map\n",
      "ALSA lib pcm_dmix.c:1052:(snd_pcm_dmix_open) unable to open slave\n"
     ]
    }
   ],
   "source": [
    "# Show all microphone with the format supported\n",
    "p = pyaudio.PyAudio()\n",
    "\n",
    "for i in range(p.get_device_count()):\n",
    "    devinfo = p.get_device_info_by_index(i)  # Or whatever device you care about.\n",
    "    try:\n",
    "        if p.is_format_supported(48000,  # Sample rate\n",
    "                                 input_device=devinfo['index'],\n",
    "                                 input_channels=devinfo['maxInputChannels'],\n",
    "                                 input_format=pyaudio.paInt16):\n",
    "            print(p.get_device_info_by_index(i).get('name'))\n",
    "    except Exception as e:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3acb8dd",
   "metadata": {},
   "source": [
    "## Criar dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36fcf3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_clean_text = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb18aab",
   "metadata": {},
   "source": [
    "### Carregar documento e separar em frases\n",
    "\n",
    "**OBS:** Este código deve ser customizado conforme cada documento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400b8905",
   "metadata": {},
   "source": [
    "Limpado os dados removendo caracteres que ocorreram com a transformação de documento para texto dentro do Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93f0850",
   "metadata": {},
   "source": [
    "#### PDF: Soltando a imagenição lendas e contos infantis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4bd0e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(extracted_text):\n",
    "    text = extracted_text.replace('-\\n', '')\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.replace(' -', '')\n",
    "    text = text.replace('— ', '')\n",
    "    text = text[:text.rfind(' soltando a imaginação soltando_a_imaginacao_c')]\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114456f3",
   "metadata": {},
   "source": [
    "Lê PDF, separa texto e faz tratamento para ser utilizado como guia para falar e criar um dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7917951",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_tale_name = 'temp'\n",
    "#all_clean_text[current_tale_name] = ''\n",
    "new_tale = False\n",
    "with pdfplumber.open('01. Soltando a imaginação lendas e contos infantis autor Hans Christian Andersen e Oscar Wilde.pdf') as pdf:\n",
    "    # Range de paginas que serao usadas\n",
    "    for i in range(8, 98):\n",
    "        # Segunda pagina do inicio de um conto eh branca\n",
    "        if new_tale:\n",
    "            new_tale = False\n",
    "            continue\n",
    "        text = pdf.pages[i]\n",
    "        # Pega somente texto normal (ignora texto em negrito)\n",
    "        text_filtered = text.filter(lambda obj: obj[\"object_type\"] == \"char\" and not \"Bold\" in obj[\"fontname\"])\n",
    "        text_cleaned = clean_text(text_filtered.extract_text())\n",
    "        # Detecta que comecou um conto novo\n",
    "        if text_cleaned.startswith('tradução do conto'):\n",
    "            new_tale = True\n",
    "            title = text.filter(lambda obj: obj[\"object_type\"] == \"char\" and \"Bold\" in obj[\"fontname\"]).extract_text()\n",
    "            current_tale_name = title[:title.find('Tradução')-1]\n",
    "            all_clean_text[current_tale_name] = ''\n",
    "            continue\n",
    "        \n",
    "        all_clean_text[current_tale_name] += f' {clean_text(text_filtered.extract_text())}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c645123e",
   "metadata": {},
   "source": [
    "#### PDF: Contos tradicionais, fábulas, lendas e mitos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ceb30c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(extracted_text):\n",
    "    text = extracted_text.replace('\\n— ', ' ')\n",
    "    text = text.replace('-\\n', '')\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.replace('— ', '')\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363ecd22",
   "metadata": {},
   "source": [
    "Lê PDF, separa texto e faz tratamento para ser utilizado como guia para falar e criar um dataset.\n",
    "\n",
    "Não separei os contos e eliminei todos títulos se quiser acompanhar qual história é, recomendo seguir com o PDF ou melhorar o código :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c21012aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_tale_name = 'contos_tradicionais'\n",
    "all_clean_text[current_tale_name] = ''\n",
    "with pdfplumber.open('03. Contos tradicionais, fábulas, lendas e mitos autor Ana Rosa Abreu, Claudia Rosenberg Aratangy, Eliane Mingues, Marília Costa Dias, Marta Durante e Telma Weisz.pdf') as pdf:\n",
    "    for i in range(6, 126):\n",
    "        text = pdf.pages[i]\n",
    "        #print(text.extract_text())\n",
    "        text_filtered = text.filter(lambda obj: obj[\"object_type\"] == \"char\" and not \"Bold\" in obj[\"fontname\"])\n",
    "        all_clean_text[current_tale_name] += f' {clean_text(text_filtered.extract_text())}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d6fb23",
   "metadata": {},
   "source": [
    "### Carrega microfone e suas dependências\n",
    "\n",
    "Para facilitar a captura correta do áudio, está sendo utilizado Wav2Vec2 para ajudar a separar ruído de fala humana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8f6ba54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_microphones(pyaudio_instance):\n",
    "    info = pyaudio_instance.get_host_api_info_by_index(0)\n",
    "    numdevices = info.get('deviceCount')\n",
    "\n",
    "    result = []\n",
    "    for i in range(0, numdevices):\n",
    "        if (pyaudio_instance.get_device_info_by_host_api_device_index(0, i).get('maxInputChannels')) > 0:\n",
    "            name = pyaudio_instance.get_device_info_by_host_api_device_index(\n",
    "                0, i).get('name')\n",
    "            result += [[i, name]]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d00eb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_device_id(device_name, microphones):\n",
    "    for device in microphones:\n",
    "        if device_name in device[1]:\n",
    "            return device[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f27c56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vad = webrtcvad.Vad()\n",
    "vad.set_mode(1)\n",
    "\n",
    "audio = pyaudio.PyAudio()\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "RATE = 48000\n",
    "# A frame must be either 10, 20, or 30 ms in duration for webrtcvad\n",
    "FRAME_DURATION = 30\n",
    "CHUNK = int(RATE * FRAME_DURATION / 1000)\n",
    "\n",
    "device_name = 'HyperX SoloCast: USB Audio (hw:2,0)' #\"default\"\n",
    "#asr_input_queue = Queue()\n",
    "microphones = list_microphones(audio)\n",
    "selected_input_device_id = get_input_device_id(\n",
    "    device_name, microphones)\n",
    "\n",
    "stream = audio.open(input_device_index=selected_input_device_id,\n",
    "                    format=FORMAT,\n",
    "                    channels=CHANNELS,\n",
    "                    rate=RATE,\n",
    "                    input=True,\n",
    "                    frames_per_buffer=CHUNK)\n",
    "stream.stop_stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c5f277",
   "metadata": {},
   "source": [
    "### Carrega modelo Wav2Vec2\n",
    "\n",
    "Código fonte: https://github.com/oliverguhr/wav2vec2-live"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a48a1d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wave2Vec2Inference:\n",
    "    def __init__(self,model_name, hotwords=[], use_lm_if_possible=True, use_gpu=True):\n",
    "        self.device = \"cuda\" if use_gpu and torch.cuda.is_available() else \"cpu\"\n",
    "        if use_lm_if_possible:            \n",
    "            self.processor = AutoProcessor.from_pretrained(model_name)\n",
    "        else:\n",
    "            self.processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCTC.from_pretrained(model_name)\n",
    "        self.model.to(self.device)\n",
    "        self.hotwords = hotwords\n",
    "        self.use_lm_if_possible = use_lm_if_possible\n",
    "\n",
    "    def buffer_to_text(self, audio_buffer):\n",
    "        if len(audio_buffer) == 0:\n",
    "            return \"\"\n",
    "\n",
    "        inputs = self.processor(torch.tensor(audio_buffer), sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(inputs.input_values.to(self.device),\n",
    "                                attention_mask=inputs.attention_mask.to(self.device)).logits            \n",
    "\n",
    "        if hasattr(self.processor, 'decoder') and self.use_lm_if_possible:\n",
    "            transcription = \\\n",
    "                self.processor.decode(logits[0].cpu().numpy(),                                      \n",
    "                                      hotwords=self.hotwords,\n",
    "                                      #hotword_weight=self.hotword_weight,  \n",
    "                                      output_word_offsets=True,                                      \n",
    "                                   )                             \n",
    "            confidence = transcription.lm_score / len(transcription.text.split(\" \"))\n",
    "            transcription = transcription.text       \n",
    "        else:\n",
    "            predicted_ids = torch.argmax(logits, dim=-1)\n",
    "            transcription = self.processor.batch_decode(predicted_ids)[0]\n",
    "            confidence = self.confidence_score(logits,predicted_ids)\n",
    "\n",
    "        return transcription, confidence   \n",
    "\n",
    "    def confidence_score(self, logits, predicted_ids):\n",
    "        scores = torch.nn.functional.softmax(logits, dim=-1)                                                           \n",
    "        pred_scores = scores.gather(-1, predicted_ids.unsqueeze(-1))[:, :, 0]\n",
    "        mask = torch.logical_and(\n",
    "            predicted_ids.not_equal(self.processor.tokenizer.word_delimiter_token_id), \n",
    "            predicted_ids.not_equal(self.processor.tokenizer.pad_token_id))\n",
    "\n",
    "        character_scores = pred_scores.masked_select(mask)\n",
    "        total_average = torch.sum(character_scores) / len(character_scores)\n",
    "        return total_average\n",
    "\n",
    "    def file_to_text(self, filename):\n",
    "        audio_input, samplerate = sf.read(filename)\n",
    "        assert samplerate == 16000\n",
    "        return self.buffer_to_text(audio_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d76d937",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "asr = Wave2Vec2Inference(\"facebook/wav2vec2-large-xlsr-53-portuguese\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3eb879b",
   "metadata": {},
   "source": [
    "## Função para escutar microfone e ignorar ruídos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4401c88e",
   "metadata": {},
   "source": [
    "Faz a captura do áudio e utiliza algumas regras para evitar pegar ruído, assim sendo possível salvar somente a fala, mesmo que a pessoa demore para começar a falar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42c2ec6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time without speaking to consider that already said all phase\n",
    "silence_time = 0.4\n",
    "def listen_speech(text, df, file_id, path_to_save_data='data'):\n",
    "    print(text)\n",
    "    while True:\n",
    "        stream.start_stream()\n",
    "        tic = time.time()\n",
    "        frames = []\n",
    "        f = b''\n",
    "        while True:\n",
    "            frame = stream.read(CHUNK, exception_on_overflow=False)\n",
    "            is_speech = vad.is_speech(frame, RATE)\n",
    "            if is_speech:\n",
    "                frames.append(frame)\n",
    "                f += frame\n",
    "                tic = time.time()\n",
    "            elif time.time() - tic < silence_time:\n",
    "                continue\n",
    "            else:\n",
    "                if len(frames) > 1:\n",
    "                    if RATE == 16000:\n",
    "                        audio_frames = f\n",
    "                        float64_buffer = np.frombuffer(audio_frames, dtype=np.int16) / 32767\n",
    "                        output_model = asr.buffer_to_text(float64_buffer)\n",
    "                    else:\n",
    "                        waveFile = wave.open(f'{path_to_save_data}/audio/temp/temp.wav', 'wb')\n",
    "                        waveFile.setnchannels(CHANNELS)\n",
    "                        waveFile.setsampwidth(audio.get_sample_size(FORMAT))\n",
    "                        #waveFile.setframerate(RATE)\n",
    "                        waveFile.setframerate(RATE)\n",
    "                        waveFile.writeframes(b''.join(frames))\n",
    "                        duration_seconds = waveFile.getnframes() / waveFile.getframerate()\n",
    "                        #print(f'Video Duration: {duration_seconds}')\n",
    "                        waveFile.close()\n",
    "\n",
    "                        audio_file, _ = librosa.load(f'{path_to_save_data}/audio/temp/temp.wav', sr=16000)\n",
    "                        output_model = asr.buffer_to_text(audio_file)\n",
    "                        #print(output_model)\n",
    "                    \n",
    "                    # Min audio size and confidence\n",
    "                    if len(output_model[0]) < 2 or output_model[1] < 0.8:\n",
    "                        #print(f'Not saved: {output_model}')\n",
    "                        frames = []\n",
    "                        tic = time.time()\n",
    "                        continue\n",
    "                    print(f'Wav2Vec2 result: {output_model}')\n",
    "                    stream.stop_stream()\n",
    "\n",
    "                    # Converte audio do microfone em formato WAV e salva em disco\n",
    "                    waveFile = wave.open(f'{path_to_save_data}/audio/{file_id}.wav', 'wb')\n",
    "                    waveFile.setnchannels(CHANNELS)\n",
    "                    waveFile.setsampwidth(audio.get_sample_size(FORMAT))\n",
    "                    #waveFile.setframerate(RATE)\n",
    "                    waveFile.setframerate(RATE)\n",
    "                    waveFile.writeframes(b''.join(frames))\n",
    "                    duration_seconds = waveFile.getnframes() / waveFile.getframerate()\n",
    "                    print(f'Video Duration: {duration_seconds}')\n",
    "                    waveFile.close()\n",
    "                    break\n",
    "                frames = []\n",
    "                f = b''\n",
    "        #stream.stop_stream()\n",
    "        #stream.close()\n",
    "        #audio.terminate()\n",
    "        result_input = input()\n",
    "        if result_input == 'r':\n",
    "            continue\n",
    "        # Jump == Do not save this audio\n",
    "        elif result_input == 'j':\n",
    "            os.remove(f'{path_to_save_data}/audio/{file_id}.wav')\n",
    "            IPython.display.clear_output()\n",
    "            return False, df, result_input\n",
    "        else:\n",
    "            IPython.display.clear_output()\n",
    "            df = pd.concat([df, pd.DataFrame([[f'{path_to_save_data}/audio/{file_id}.wav', text.strip()]], columns=list(df.columns))])\n",
    "            return True, df, result_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6b32fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stream.stop_stream()\n",
    "#stream.close()\n",
    "#audio.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27943bc",
   "metadata": {},
   "source": [
    "### Mostra frases e captura microfone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57e0a541",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_annotation(path='data'):\n",
    "    if os.path.exists(f'{path}/annotation.tsv'):\n",
    "        return pd.read_csv(f'{path}/annotation.tsv', sep='\\t')\n",
    "    return pd.DataFrame(columns=['path', 'sentence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba83435",
   "metadata": {},
   "source": [
    "Cada áudio possi um ID único. Quando começar a capturar precisa verificar se já existem áudios e caso positivo precisa continuar a contagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38a42b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_file_id(df_annotation):\n",
    "    if len(df_annotation) > 0:\n",
    "        last_filename = df_annotation.iloc[-1][0]\n",
    "        if last_filename.rfind('/') == -1:\n",
    "            return int(last_filename[:last_filename.find('.')]) + 1\n",
    "        return int(last_filename[last_filename.rfind('/')+1:last_filename.find('.')]) + 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d9973a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_annotation(is_to_save, df_annotation, path_to_save_data):\n",
    "    if is_to_save:\n",
    "        df_annotation.to_csv(f'{path_to_save_data}/annotation.tsv', sep='\\t', index=None)\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c4bb3a",
   "metadata": {},
   "source": [
    "Procura as ultmas N frases iguais as salvas no arquivo de anotação para continuar de onde parou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2bdccd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_search_where_stopped_default = 1\n",
    "def check_is_where_stopped(text, count_search_where_stopped):\n",
    "    if len(df_annotation) > 0:\n",
    "        if text.strip() == df_annotation.iloc[-count_search_where_stopped].sentence:\n",
    "            count_search_where_stopped -= 1\n",
    "            if count_search_where_stopped == 0:\n",
    "                return True, count_search_where_stopped\n",
    "        else:\n",
    "            count_search_where_stopped = count_search_where_stopped_default\n",
    "    \n",
    "    return False, count_search_where_stopped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59a8bbf",
   "metadata": {},
   "source": [
    "Aqui é onde a mágico acontece. Será mostrado uma frase no output e iniciado a captura do microfone, quando for detectado que algo foi falado (e parou de falar) será aberto um input, podendo ter 4 ações possíveis:\n",
    "\n",
    "- Deixar em branco = Salva áudio e anotação\n",
    "- Digitar a letra \"j\" = Ignora frase, não salva áudio nem anotação\n",
    "- Digitar a letra \"r\" = Para falar no microfone a mesma frase (ignorando o áudio anterior). Para quando falou algo errado\n",
    "- Digitar \"end\" = Para parar de mostrar novas frases e encerrar o sistema, salvando o último áudio falado (apenas encerra se a frase terminar com ponto final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b96e15c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/max/anaconda3/envs/audiowav/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3516: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "is_where_stopped = True  # True == start from beginning; False == Continue from where stopped\n",
    "\n",
    "path_to_save_data = 'data'\n",
    "pathlib.Path(path_to_save_data).mkdir(exist_ok=True, parents=True)\n",
    "pathlib.Path(path_to_save_data + '/audio').mkdir(exist_ok=True, parents=True)\n",
    "pathlib.Path(path_to_save_data + '/audio/temp').mkdir(exist_ok=True, parents=True)\n",
    "df_annotation = get_df_annotation(path_to_save_data)\n",
    "file_id = get_next_file_id(df_annotation)\n",
    "result_input = ''\n",
    "\n",
    "count_search_where_stopped = count_search_where_stopped_default\n",
    "for tale_name, tale_text in all_clean_text.items():\n",
    "    for text in tale_text.split('.'):\n",
    "        if len(text) > 0:\n",
    "            is_subsplited = False\n",
    "            for character_split in [';', '!', '?']:\n",
    "                if text.find(character_split) != -1:\n",
    "                    splited = text.split(character_split)\n",
    "                    for idx, text2 in enumerate(splited):\n",
    "                        if len(text2) > 0:\n",
    "                            # Add the character removed by split\n",
    "                            if idx+1 != len(splited):\n",
    "                                text2 += character_split\n",
    "                            else:\n",
    "                                text2 += '.'\n",
    "                            \n",
    "                            if is_where_stopped:\n",
    "                                is_where_stopped, count_search_where_stopped = check_is_where_stopped(text2, count_search_where_stopped)\n",
    "                                continue\n",
    "                            \n",
    "                                \n",
    "                            is_file_saved, df_annotation, result_input = listen_speech(text2, df_annotation, file_id, path_to_save_data)\n",
    "                            file_id += save_annotation(is_file_saved, df_annotation, path_to_save_data)\n",
    "                    is_subsplited = True\n",
    "                    break\n",
    "            \n",
    "            if is_subsplited is False:\n",
    "                if is_where_stopped is False:\n",
    "                    is_where_stopped, count_search_where_stopped = check_is_where_stopped(text + '.', count_search_where_stopped)\n",
    "                    continue\n",
    "                \n",
    "                is_file_saved, df_annotation, result_input = listen_speech(text + '.', df_annotation, file_id, path_to_save_data)\n",
    "                file_id += save_annotation(is_file_saved, df_annotation, path_to_save_data)\n",
    "                \n",
    "            if result_input == 'end':\n",
    "                sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a829d90",
   "metadata": {},
   "source": [
    "## Calcula tempo audios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0db86dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total audio files: 8\n",
      "Mean time audio files (sec): 3.1575\n",
      "Total time audio files (min): 0.42100000000000004\n"
     ]
    }
   ],
   "source": [
    "path_data = 'data/audio'\n",
    "\n",
    "total_time = 0\n",
    "total_files = 0\n",
    "for filename in os.listdir(path_data):\n",
    "    if filename.endswith('wav'):\n",
    "        audio_file, _ = librosa.load(f'{path_data}/{filename}', sr=16000)\n",
    "        total_time += librosa.get_duration(y=audio_file, sr=16000)\n",
    "        total_files += 1\n",
    "print(f'Total audio files: {total_files}')\n",
    "print(f'Mean time audio files (sec): {total_time/total_files}')\n",
    "print(f'Total time audio files (min): {total_time/60}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adac2515",
   "metadata": {},
   "source": [
    "## Preparar dataset para treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8cb8f456",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ddd7222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_annotation(path='data'):\n",
    "    if os.path.exists(f'{path}/annotation.tsv'):\n",
    "        return pd.read_csv(f'{path}/annotation.tsv', sep='\\t')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c48cd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset_splited(df_annotation, filename='train'):\n",
    "    df_annotation.to_csv(f'{filename}.tsv', sep='\\t', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "91bcc507",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annotation = get_df_annotation('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "88f990e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df_annotation, test_size=0.4)\n",
    "test, validation = train_test_split(test, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9bd9ddcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dataset_splited(train, 'train')\n",
    "save_dataset_splited(test, 'test')\n",
    "save_dataset_splited(validation, 'validation')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
